{
  "data": {
    "query": "data",
    "results": [
      {
        "id": "ri-1",
        "title": "Information Leaked To Hosted Model",
        "filename": "ri-1_information-leaked-to-hosted-model.md",
        "snippet": "## Summary\n\nUsing third-party hosted LLMs creates a **two-way trust boundary** where neither inputs nor outputs can be fully trusted. Sensitive financial data sent for inference may be memorized by mo...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-1_information-leaked-to-hosted-model.md"
      },
      {
        "id": "ri-2",
        "title": "Information Leaked to Vector Store",
        "filename": "ri-2_information-leaked-to-vector-store.md",
        "snippet": "## Summary\n\nLLM applications pose data leakage risks not only through vector stores but across all components handling derived data, such as embeddings, prompt logs, and caches. These representations,...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-2_information-leaked-to-vector-store.md"
      },
      {
        "id": "ri-4",
        "title": "Hallucination and Inaccurate Outputs",
        "filename": "ri-4_hallucination-and-inaccurate-outputs.md",
        "snippet": "## Summary\n\nLLM hallucinations occur when a model generates confident but incorrect or fabricated information due to its reliance on statistical patterns rather than factual understanding. Techniques ...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-4_hallucination-and-inaccurate-outputs.md"
      },
      {
        "id": "ri-5",
        "title": "Foundation Model Versioning",
        "filename": "ri-5_foundation-model-versioning.md",
        "snippet": "## Summary\n\nFoundation model instability refers to unpredictable changes in model behavior over time due to external factors like version updates, system prompt modifications, or provider changes. Unl...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-5_foundation-model-versioning.md"
      },
      {
        "id": "ri-7",
        "title": "Availability of Foundational Model",
        "filename": "ri-7_availability-of-foundational-model.md",
        "snippet": "## Summary\n\nFoundation models often rely on GPU-heavy infrastructure hosted by third-party providers, introducing risks related to service availability and performance. Key threats include Denial of W...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-7_availability-of-foundational-model.md"
      },
      {
        "id": "ri-8",
        "title": "Tampering With the Foundational Model",
        "filename": "ri-8_tampering-with-the-foundational-model.md",
        "snippet": "## Summary\n\nFoundational models provided by third-party SaaS vendors are vulnerable to supply chain risks, including tampering with training data, model weights, or infrastructure components such as G...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-8_tampering-with-the-foundational-model.md"
      },
      {
        "id": "ri-9",
        "title": "Data Poisoning",
        "filename": "ri-9_data-poisoning.md",
        "snippet": "## Summary\n\nData poisoning occurs when adversaries tamper with training or fine-tuning data to manipulate an AI model\u2019s behaviour, often by injecting misleading or malicious patterns. This can lead to...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-9_data-poisoning.md"
      },
      {
        "id": "ri-10",
        "title": "Prompt Injection",
        "filename": "ri-10_prompt-injection.md",
        "snippet": "## Summary\n\nPrompt injection occurs when attackers craft inputs that manipulate a language model into producing unintended, harmful, or unauthorized outputs. These attacks can be direct\u2014overriding the...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-10_prompt-injection.md"
      },
      {
        "id": "ri-14",
        "title": "Inadequate System Alignment",
        "filename": "ri-14_inadequate-system-alignment.md",
        "snippet": "## Summary\n\nLLM-powered RAG systems may generate responses that diverge from their intended business purpose, producing outputs that appear relevant but contain inaccurate financial advice, biased rec...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-14_inadequate-system-alignment.md"
      },
      {
        "id": "ri-16",
        "title": "Bias and Discrimination",
        "filename": "ri-16_bias-and-discrimination.md",
        "snippet": "## Summary\n\nAI systems can systematically disadvantage protected groups through biased training data, flawed design, or proxy variables that correlate with sensitive characteristics. In financial serv...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-16_bias-and-discrimination.md"
      },
      {
        "id": "ri-19",
        "title": "Data Quality and Drift",
        "filename": "ri-19_data-quality-and-drift.md",
        "snippet": "## Summary\n\nGenerative AI systems rely heavily on the quality and freshness of their training data, and outdated or poor-quality data can lead to inaccurate, biased, or irrelevant outputs. In fast-mov...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-19_data-quality-and-drift.md"
      },
      {
        "id": "ri-23",
        "title": "Intellectual Property (IP) and Copyright",
        "filename": "ri-23_intellectual-property-ip-and-copyright.md",
        "snippet": "## Summary\n\nGenerative AI models may be trained on copyrighted or proprietary material, raising the risk that outputs could unintentionally infringe on intellectual property rights. In financial servi...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-23_intellectual-property-ip-and-copyright.md"
      }
    ],
    "count": 12
  },
  "model": {
    "query": "model",
    "results": [
      {
        "id": "ri-1",
        "title": "Information Leaked To Hosted Model",
        "filename": "ri-1_information-leaked-to-hosted-model.md",
        "snippet": "## Summary\n\nUsing third-party hosted LLMs creates a **two-way trust boundary** where neither inputs nor outputs can be fully trusted. Sensitive financial data sent for inference may be memorized by mo...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-1_information-leaked-to-hosted-model.md"
      },
      {
        "id": "ri-2",
        "title": "Information Leaked to Vector Store",
        "filename": "ri-2_information-leaked-to-vector-store.md",
        "snippet": "## Summary\n\nLLM applications pose data leakage risks not only through vector stores but across all components handling derived data, such as embeddings, prompt logs, and caches. These representations,...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-2_information-leaked-to-vector-store.md"
      },
      {
        "id": "ri-4",
        "title": "Hallucination and Inaccurate Outputs",
        "filename": "ri-4_hallucination-and-inaccurate-outputs.md",
        "snippet": "## Summary\n\nLLM hallucinations occur when a model generates confident but incorrect or fabricated information due to its reliance on statistical patterns rather than factual understanding. Techniques ...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-4_hallucination-and-inaccurate-outputs.md"
      },
      {
        "id": "ri-5",
        "title": "Foundation Model Versioning",
        "filename": "ri-5_foundation-model-versioning.md",
        "snippet": "## Summary\n\nFoundation model instability refers to unpredictable changes in model behavior over time due to external factors like version updates, system prompt modifications, or provider changes. Unl...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-5_foundation-model-versioning.md"
      },
      {
        "id": "ri-6",
        "title": "Non-Deterministic Behaviour",
        "filename": "ri-6_non-deterministic-behaviour.md",
        "snippet": "## Summary\n\nLLMs exhibit non-deterministic behaviour, meaning they can generate different outputs for the same input due to probabilistic sampling and internal variability. This unpredictability can l...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-6_non-deterministic-behaviour.md"
      },
      {
        "id": "ri-7",
        "title": "Availability of Foundational Model",
        "filename": "ri-7_availability-of-foundational-model.md",
        "snippet": "## Summary\n\nFoundation models often rely on GPU-heavy infrastructure hosted by third-party providers, introducing risks related to service availability and performance. Key threats include Denial of W...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-7_availability-of-foundational-model.md"
      },
      {
        "id": "ri-8",
        "title": "Tampering With the Foundational Model",
        "filename": "ri-8_tampering-with-the-foundational-model.md",
        "snippet": "## Summary\n\nFoundational models provided by third-party SaaS vendors are vulnerable to supply chain risks, including tampering with training data, model weights, or infrastructure components such as G...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-8_tampering-with-the-foundational-model.md"
      },
      {
        "id": "ri-9",
        "title": "Data Poisoning",
        "filename": "ri-9_data-poisoning.md",
        "snippet": "## Summary\n\nData poisoning occurs when adversaries tamper with training or fine-tuning data to manipulate an AI model\u2019s behaviour, often by injecting misleading or malicious patterns. This can lead to...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-9_data-poisoning.md"
      },
      {
        "id": "ri-10",
        "title": "Prompt Injection",
        "filename": "ri-10_prompt-injection.md",
        "snippet": "## Summary\n\nPrompt injection occurs when attackers craft inputs that manipulate a language model into producing unintended, harmful, or unauthorized outputs. These attacks can be direct\u2014overriding the...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-10_prompt-injection.md"
      },
      {
        "id": "ri-14",
        "title": "Inadequate System Alignment",
        "filename": "ri-14_inadequate-system-alignment.md",
        "snippet": "## Summary\n\nLLM-powered RAG systems may generate responses that diverge from their intended business purpose, producing outputs that appear relevant but contain inaccurate financial advice, biased rec...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-14_inadequate-system-alignment.md"
      },
      {
        "id": "ri-16",
        "title": "Bias and Discrimination",
        "filename": "ri-16_bias-and-discrimination.md",
        "snippet": "## Summary\n\nAI systems can systematically disadvantage protected groups through biased training data, flawed design, or proxy variables that correlate with sensitive characteristics. In financial serv...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-16_bias-and-discrimination.md"
      },
      {
        "id": "ri-17",
        "title": "Lack of Explainability",
        "filename": "ri-17_lack-of-explainability.md",
        "snippet": "## Summary\n\nAI systems, particularly those using complex foundation models, often lack transparency, making it difficult to interpret how decisions are made. This limits firms\u2019 ability to explain outc...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-17_lack-of-explainability.md"
      },
      {
        "id": "ri-18",
        "title": "Model Overreach / Expanded Use",
        "filename": "ri-18_model-overreach-expanded-use.md",
        "snippet": "## Summary\n\nModel overreach occurs when AI systems are used beyond their intended purpose, often due to overconfidence in their capabilities. This can lead to poor-quality, non-compliant, or misleadin...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-18_model-overreach-expanded-use.md"
      },
      {
        "id": "ri-19",
        "title": "Data Quality and Drift",
        "filename": "ri-19_data-quality-and-drift.md",
        "snippet": "## Summary\n\nGenerative AI systems rely heavily on the quality and freshness of their training data, and outdated or poor-quality data can lead to inaccurate, biased, or irrelevant outputs. In fast-mov...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-19_data-quality-and-drift.md"
      },
      {
        "id": "ri-20",
        "title": "Reputational Risk",
        "filename": "ri-20_reputational-risk.md",
        "snippet": "## Summary\n\nAI failures or misuse\u2014especially in customer-facing systems\u2014can quickly escalate into public incidents that damage a firm\u2019s reputation and erode trust. Inaccurate, offensive, or unfair out...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-20_reputational-risk.md"
      },
      {
        "id": "ri-22",
        "title": "Regulatory Compliance and Oversight",
        "filename": "ri-22_regulatory-compliance-and-oversight.md",
        "snippet": "## Summary\n\nAI systems in financial services must comply with the same regulatory standards as human-driven processes, including those related to suitability, fairness, record-keeping, and marketing c...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-22_regulatory-compliance-and-oversight.md"
      },
      {
        "id": "ri-23",
        "title": "Intellectual Property (IP) and Copyright",
        "filename": "ri-23_intellectual-property-ip-and-copyright.md",
        "snippet": "## Summary\n\nGenerative AI models may be trained on copyrighted or proprietary material, raising the risk that outputs could unintentionally infringe on intellectual property rights. In financial servi...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-23_intellectual-property-ip-and-copyright.md"
      }
    ],
    "count": 17
  },
  "injection": {
    "query": "injection",
    "results": [
      {
        "id": "ri-10",
        "title": "Prompt Injection",
        "filename": "ri-10_prompt-injection.md",
        "snippet": "## Summary\n\nPrompt injection occurs when attackers craft inputs that manipulate a language model into producing unintended, harmful, or unauthorized outputs. These attacks can be direct\u2014overriding the...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-10_prompt-injection.md"
      },
      {
        "id": "ri-14",
        "title": "Inadequate System Alignment",
        "filename": "ri-14_inadequate-system-alignment.md",
        "snippet": "## Summary\n\nLLM-powered RAG systems may generate responses that diverge from their intended business purpose, producing outputs that appear relevant but contain inaccurate financial advice, biased rec...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-14_inadequate-system-alignment.md"
      }
    ],
    "count": 2
  },
  "privacy": {
    "query": "privacy",
    "results": [
      {
        "id": "ri-1",
        "title": "Information Leaked To Hosted Model",
        "filename": "ri-1_information-leaked-to-hosted-model.md",
        "snippet": "## Summary\n\nUsing third-party hosted LLMs creates a **two-way trust boundary** where neither inputs nor outputs can be fully trusted. Sensitive financial data sent for inference may be memorized by mo...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-1_information-leaked-to-hosted-model.md"
      },
      {
        "id": "ri-5",
        "title": "Foundation Model Versioning",
        "filename": "ri-5_foundation-model-versioning.md",
        "snippet": "## Summary\n\nFoundation model instability refers to unpredictable changes in model behavior over time due to external factors like version updates, system prompt modifications, or provider changes. Unl...",
        "url": "https://raw.githubusercontent.com/finos/ai-governance-framework/main/docs/_risks/ri-5_foundation-model-versioning.md"
      }
    ],
    "count": 2
  },
  "nonexistent_term_12345": {
    "query": "nonexistent_term_12345",
    "results": [],
    "count": 0
  }
}
